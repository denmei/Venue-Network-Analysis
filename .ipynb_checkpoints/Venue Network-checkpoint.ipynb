{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set(rc={'figure.figsize':(12,8)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venue Network Analysis\n",
    "\n",
    "Understanding why Treatwell users choose particular salons is crucial to optimize different aspects of our web appearance. There are many factors a user could consider before booking with a particular salon: treatment type, price, location and the salon's rating are just a few examples. The question is, which of all these factors are the most important ones.\n",
    "\n",
    "A possible approach to answer this question is looking at the venues' customer circles. We can assume that a user always has the same priorities when picking a new salon. Hence salons with overlapping customer circles should share these attributes that are so important to their clienteles. Accordingly, clustering venues with similar customs and checking for shared attributes will give us an idea about what made these customers choose these salons.\n",
    "\n",
    "This notebook builds the network of Treatwell's salons in London, using historic venue and order data. The table can then be loaded into Gephi, which allows visualizing and exploring the network.\n",
    "\n",
    "Read my [article on Medium](link) to get a full overview of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading order data\n",
    "venue_df = pd.read_csv('venues_df.csv')\n",
    "venue_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading venue treatment type data \n",
    "venue_tt_df = pd.read_csv('venues_tt_df.csv')\n",
    "venue_tt_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(venue_df))\n",
    "# print(venue_df.head(5))\n",
    "\n",
    "users_per_venue = venue_df[['venue_details_key', 'customer_account_id']] \\\n",
    "    .groupby(['venue_details_key'], as_index=False) \\\n",
    "    .count()\n",
    "\n",
    "nr = list(users_per_venue['customer_account_id'])\n",
    "perc = np.percentile(nr, 33)\n",
    "print(perc)\n",
    "\n",
    "users_per_venue = users_per_venue[users_per_venue['customer_account_id'] >= 50]\n",
    "users_per_venue.rename(columns={'customer_account_id':'customer_count'}, inplace=True)\n",
    "\n",
    "venue_df_small = venue_df.merge(users_per_venue, on='venue_details_key', how='inner')\n",
    "\n",
    "print(len(venue_df_small))\n",
    "# print(venue_df_small.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify salons with similar customer circles, an eligible metric is needed to measure the similarity between two sets. A popular and straight forward measure for cases like this is the Jaccard Coefficient.\n",
    "\n",
    "Unfortunately, the index has a critical drawback since it ranks connections between two entities with an equal degree higher. Imagine the situation in the figure below, with a very big salon (S1) whose customer circle overlaps with many other salons and a very small salon (S2) whose clientele overlaps with the one of S1, but to a very high degree. Obviously we wanted this connection to be ranked very high. But since S1 shares customers with so many other venues, the score would be fairly low.\n",
    "\n",
    "Therefore, I defined an alternative metric to measure the similarity between to venues. Using this metric, the similarity between S1 and S2, where the connection is only meaningful for one of the parties, would be around 0.5. If the connection is important for both sides, the value converges to 1 while it approximates 0 for very weak relations on both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    \"\"\"\n",
    "    Calculates jaccard similarity for two lists.\n",
    "    \"\"\"\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection / union)\n",
    "\n",
    "def similarity_two_sided(list1, list2):\n",
    "    \"\"\"\n",
    "    Alternative approach to calculate similarity of two lists.\n",
    "    \"\"\"\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union_1 = float(intersection / len(list_1))\n",
    "    union_2 = float(intersection / len(list_2))\n",
    "    return (union_1 + union_2)/2\n",
    "\n",
    "venue_ids = set(venue_df_small['venue_details_key'])\n",
    "\n",
    "venue_connections = []\n",
    "right_venue_list = []\n",
    "left_venue_list = []\n",
    "\n",
    "# build dictionary containing the customers for every salon\n",
    "venues_with_customers = dict()\n",
    "for venue_id in venue_ids:\n",
    "    venue_customers = set(venue_df_small[venue_df_small['venue_details_key'] == venue_id]['customer_account_id'])\n",
    "    venues_with_customers[venue_id] = venue_customers\n",
    "\n",
    "# calculate similarity for every venue combination\n",
    "used = []\n",
    "for left_venue_id in venue_ids:\n",
    "    for right_venue_id in venue_ids:\n",
    "        if(right_venue_id) not in used:\n",
    "            similarity = similarity_two_sided(venues_with_customers[left_venue_id], venues_with_customers[right_venue_id])\n",
    "            venue_connections += [similarity]\n",
    "            right_venue_list += [right_venue_id]\n",
    "            left_venue_list += [left_venue_id]\n",
    "    used += [left_venue_id]\n",
    "\n",
    "# combine the three lists in a dataframe (ultimately the network table)\n",
    "similarity_df = pd.DataFrame()\n",
    "similarity_df['source'] = left_venue_list\n",
    "similarity_df['target'] = right_venue_list\n",
    "similarity_df['weight'] = venue_connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add venue names, drop irrelevant rows\n",
    "names_ids = venue_df_small[['venue_details_key', 'venue_name']] \\\n",
    "    .rename(columns={'venue_name': 'source_name', 'venue_details_key': 'source'}) \\\n",
    "    .drop_duplicates()\n",
    "similarity_df= similarity_df.merge(names_ids, on='source', how='inner')\n",
    "\n",
    "names_ids = venue_df_small[['venue_details_key', 'venue_name']] \\\n",
    "    .rename(columns={'venue_name': 'target_name', 'venue_details_key': 'target'}) \\\n",
    "    .drop_duplicates()\n",
    "\n",
    "similarity_df= similarity_df.merge(names_ids, on='target', how='inner')\n",
    "similarity_df = similarity_df[(similarity_df['source'] != similarity_df['target']) \\\n",
    "                              & (similarity_df['weight'] > 0)]\n",
    "similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_df.sort_values('weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only most important 20% edges for each node\n",
    "similarity_df['rank'] = similarity_df.sort_values(['weight'], ascending=[False]) \\\n",
    "             .groupby(['source']) \\\n",
    "             .cumcount() + 1\n",
    "similarity_df['count'] = similarity_df.groupby(['source'])['source'].transform('count') \n",
    "print(len(similarity_df[similarity_df['count'] >= 5]))\n",
    "similarity_df['count'] = similarity_df['count'] * 0.2\n",
    "similarity_df = similarity_df[similarity_df['rank'] <= similarity_df['count']]\n",
    "print(len(similarity_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the edges table\n",
    "similarity_df['weight'] = similarity_df['weight'] * 10\n",
    "similarity_df.to_csv('/Users/dennismeisner/Documents/Venue_Network/jaccard_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a separate table with information about the network's nodes \n",
    "# (basically salon information like name, top_rated, area, average order value etc.)\n",
    "nodes = venue_df_small[['city', 'venue_details_key', 'venue_name', 'has_been_top_rated_venue', \\\n",
    "                        'order_count', 'payment_amount']]\n",
    "nodes = nodes.merge(venue_tt_df, on='venue_details_key', how='inner') \n",
    "nodes = nodes.rename(columns={'venue_details_key': 'id', 'venue_name':'label'})\n",
    "nodes = nodes.groupby(['city', 'id', 'label', 'has_been_top_rated_venue', 'treatment_type'], as_index=False) \\\n",
    "    .sum()\n",
    "nodes['avg_order_val'] = ((nodes['payment_amount'] / nodes['order_count']) /5).astype('int') * 5\n",
    "nodes['order_count'] = ((nodes['order_count']) /5).astype('int') * 5\n",
    "nodes = nodes[['city', 'id', 'label', 'has_been_top_rated_venue', 'treatment_type', 'order_count', 'avg_order_val']]\n",
    "print(nodes.head())\n",
    "nodes.to_csv('/Users/dennismeisner/Documents/Venue_Network/nodes_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
